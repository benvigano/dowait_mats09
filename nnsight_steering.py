"""
NNsight-based steering vector implementation for mistake realization.
Uses contrasting pairs of wrong baseline vs successful corrections to build steering vectors.
"""

import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import json
from cache import print_timestamped_message
from database import get_experiment_results
from low_level import evaluate_answer


class NNsightSteeringExperiment:
    """
    Steering vector experiment using nnsight for Qwen3-14B model.
    Builds vectors that encode the direction "realize you've made a mistake".
    """
    
    def __init__(self, model_interface, results_dir: str, layer: int = 20):
        """
        Initialize steering experiment.
        
        Args:
            model_interface: Model interface that supports nnsight
            results_dir: Directory containing experiment results
            layer: Layer to extract activations from and apply steering
        """
        self.model_interface = model_interface
        self.results_dir = results_dir
        self.layer = layer
        self.steering_vector = None
        self.train_data = []
        self.validation_data = []
        
    def extract_contrasting_pairs(self, train_fraction: float = 0.8) -> bool:
        """
        Extract contrasting pairs for steering vector creation.
        
        Returns:
            bool: True if sufficient contrasting pairs found
        """
        print_timestamped_message("Extracting contrasting pairs for steering vector...")
        
        # Get all experiment results from database
        results = get_experiment_results(self.results_dir)
        
        if not results:
            print_timestamped_message("‚ö†Ô∏è No experiment results found in database")
            return False
        
        # Filter for problems with anthropic-labeled wrong baselines and successful interventions
        contrasting_pairs = []
        
        for result in results:
            # Must have anthropic-labeled wrong baseline with error location
            if (result.get('baseline_correct') == 0 and 
                result.get('baseline_error_line_number') is not None and
                result.get('baseline_error_line_content') is not None and
                result.get('baseline_raw_response')):
                
                # Must have successful intervention
                if result.get('intervention_correct') == 1 and result.get('intervention_raw_response'):
                    
                    # Extract the moment right after the mistake in baseline
                    baseline_cot = result['baseline_raw_response']
                    error_line = result['baseline_error_line_content']
                    
                    # Find where the error occurs in the baseline
                    if error_line in baseline_cot:
                        error_index = baseline_cot.find(error_line)
                        error_end = error_index + len(error_line)
                        
                        # Extract the baseline continuation after the error (where it should realize the mistake)
                        baseline_continuation = baseline_cot[error_end:].strip()
                        
                        if baseline_continuation:  # Make sure there's content after the error
                            contrasting_pairs.append({
                                'problem_id': result.get('problem', 'unknown'),
                                'problem_text': result.get('problem', ''),
                                'ground_truth': result.get('ground_truth_answer', ''),
                                'error_location': error_index + len(error_line),
                                'baseline_prefix': baseline_cot[:error_end],
                                'baseline_continuation': baseline_continuation,
                                'intervention_response': result['intervention_raw_response'],
                                'error_line': error_line
                            })
        
        if len(contrasting_pairs) < 5:
            print_timestamped_message(f"‚ö†Ô∏è Only found {len(contrasting_pairs)} contrasting pairs. Need at least 5.")
            return False
        
        print_timestamped_message(f"Found {len(contrasting_pairs)} contrasting pairs")
        
        # Split into train/validation
        np.random.seed(42)
        indices = np.random.permutation(len(contrasting_pairs))
        train_size = int(len(contrasting_pairs) * train_fraction)
        
        self.train_data = [contrasting_pairs[i] for i in indices[:train_size]]
        self.validation_data = [contrasting_pairs[i] for i in indices[train_size:]]
        
        print_timestamped_message(f"Split into {len(self.train_data)} training and {len(self.validation_data)} validation pairs")
        return True
    
    def _extract_activations_from_model(self, nnsight_model, prompt: str, target_layer: int) -> torch.Tensor:
        """
        Extract activations from a pre-loaded nnsight model with memory management.
        """
        try:
            # Get tokenizer from the model interface
            if hasattr(self.model_interface, 'tokenizer') and self.model_interface.tokenizer is not None:
                tokenizer = self.model_interface.tokenizer
            elif hasattr(self.model_interface, 'hf_tokenizer') and self.model_interface.hf_tokenizer is not None:
                tokenizer = self.model_interface.hf_tokenizer
            else:
                raise ValueError("No tokenizer available")
            
            # Tokenize input and move to GPU
            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            input_ids = inputs['input_ids'].to(nnsight_model.device if hasattr(nnsight_model, 'device') else 'cuda')
            
            # Clear any cached computations in nnsight
            if hasattr(nnsight_model, 'reset'):
                nnsight_model.reset()
            
            # Force memory cleanup before tracing
            import torch
            import gc
            torch.cuda.empty_cache()
            gc.collect()
            
            with nnsight_model.trace(input_ids):
                # Extract activations from the specified layer
                # For Qwen models, get the hidden states output
                hidden_states = nnsight_model.model.layers[target_layer].output[0]
                activations = hidden_states.save()
            
            # Move to CPU immediately to free GPU memory
            activations_cpu = activations.detach().cpu()
            
            # Handle different activation shapes
            if len(activations_cpu.shape) == 3:
                # Expected shape: (batch_size, seq_len, hidden_size)
                last_token_activations = activations_cpu[:, -1, :]
            elif len(activations_cpu.shape) == 2:
                # If shape is (seq_len, hidden_size), take last position
                last_token_activations = activations_cpu[-1, :].unsqueeze(0)
            else:
                print_timestamped_message(f"Unexpected activation shape: {activations_cpu.shape}")
                return None
            
            # Clear GPU memory after each extraction
            del activations, activations_cpu, input_ids, inputs
            torch.cuda.empty_cache()
            gc.collect()
            
            return last_token_activations
            
        except OutOfMemoryError as e:
            print_timestamped_message(f"OOM during activation extraction: {e}")
            # Emergency memory cleanup
            import torch
            import gc
            torch.cuda.empty_cache()
            gc.collect()
            return None
        except Exception as e:
            print_timestamped_message(f"Error extracting activations: {e}")
            return None
    
    def get_model_activations(self, prompt: str, target_layer: int) -> torch.Tensor:
        """
        Get model activations at specified layer using nnsight.
        
        Args:
            prompt: Input prompt
            target_layer: Layer to extract activations from
            
        Returns:
            torch.Tensor: Activations at the last token position
        """
        try:
            # Use nnsight to get activations
            nnsight_model = self.model_interface.get_nnsight_model()
            if nnsight_model is None:
                raise ValueError("Model doesn't support nnsight")
            
            # Get tokenizer from the model interface
            if hasattr(self.model_interface, 'tokenizer') and self.model_interface.tokenizer is not None:
                tokenizer = self.model_interface.tokenizer
            elif hasattr(self.model_interface, 'hf_tokenizer') and self.model_interface.hf_tokenizer is not None:
                tokenizer = self.model_interface.hf_tokenizer
            else:
                raise ValueError("No tokenizer available")
            
            # Tokenize input
            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            with nnsight_model.trace(inputs['input_ids']):
                # Extract activations from the specified layer
                # For Qwen models, get the hidden states output
                hidden_states = nnsight_model.model.layers[target_layer].output[0]
                activations = hidden_states.save()
            
            # Debug: Check activation shape
            print_timestamped_message(f"Activation shape: {activations.shape}")
            
            # Handle different activation shapes
            if len(activations.shape) == 3:
                # Expected shape: (batch_size, seq_len, hidden_size)
                last_token_activations = activations[:, -1, :].detach().cpu()
            elif len(activations.shape) == 2:
                # If shape is (seq_len, hidden_size), take last position
                last_token_activations = activations[-1, :].unsqueeze(0).detach().cpu()
            else:
                print_timestamped_message(f"Unexpected activation shape: {activations.shape}")
                return None
            
            return last_token_activations
            
        except Exception as e:
            print_timestamped_message(f"Error extracting activations: {e}")
            return None
    
    def build_steering_vector(self) -> bool:
        """
        Build steering vector from contrasting activation pairs.
        Reloads model for each extraction to prevent memory leaks.
        
        Returns:
            bool: True if steering vector successfully created
        """
        print_timestamped_message(f"Building steering vector from {len(self.train_data)} contrasting pairs...")
        
        if not self.train_data:
            print_timestamped_message("‚ö†Ô∏è No training data available")
            return False
        
        baseline_activations = []
        intervention_activations = []
        
        for i, pair in enumerate(self.train_data):
            print_timestamped_message(f"Processing pair {i+1}/{len(self.train_data)}...")
            
            # Get activations for baseline (wrong continuation) 
            baseline_prompt = pair['baseline_prefix']
            baseline_acts = self.get_model_activations(baseline_prompt, self.layer)
            
            if baseline_acts is None:
                continue
            
            # Get activations for intervention (corrected response)
            intervention_acts = self.get_model_activations(pair['intervention_response'][:len(baseline_prompt)], self.layer)
            
            if intervention_acts is None:
                continue
            
            baseline_activations.append(baseline_acts)
            intervention_activations.append(intervention_acts)
        
        if len(baseline_activations) < 3:
            print_timestamped_message(f"‚ö†Ô∏è Only got {len(baseline_activations)} valid activation pairs. Need at least 3.")
            return False
        
        # Calculate steering vector as mean difference
        baseline_mean = torch.stack(baseline_activations).mean(dim=0)
        intervention_mean = torch.stack(intervention_activations).mean(dim=0)
        
        # Steering vector points from baseline (no realization) to intervention (realization)
        self.steering_vector = intervention_mean - baseline_mean
        
        # Normalize the steering vector
        self.steering_vector = self.steering_vector / torch.norm(self.steering_vector)
        
        print_timestamped_message(f"‚úÖ Steering vector created with norm {torch.norm(self.steering_vector):.4f}")
        print_timestamped_message(f"Vector shape: {self.steering_vector.shape}")
        
        return True
    
    def apply_steering_to_generation(self, prompt: str, steering_strength: float = 1.0, max_new_tokens: int = 512) -> str:
        """
        Apply steering vector during generation using nnsight.
        
        Args:
            prompt: Input prompt
            steering_strength: Multiplier for steering vector strength
            max_new_tokens: Maximum tokens to generate
            
        Returns:
            str: Generated text with steering applied
        """
        try:
            nnsight_model = self.model_interface.get_nnsight_model()
            if nnsight_model is None:
                return "Error: Model doesn't support nnsight"
            
            # Get tokenizer
            if hasattr(self.model_interface, 'tokenizer') and self.model_interface.tokenizer is not None:
                tokenizer = self.model_interface.tokenizer
            elif hasattr(self.model_interface, 'hf_tokenizer') and self.model_interface.hf_tokenizer is not None:
                tokenizer = self.model_interface.hf_tokenizer
            else:
                return "Error: No tokenizer available"
            
            # Tokenize prompt
            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            input_ids = inputs['input_ids']
            
            # For simplicity, let's use a simpler approach: 
            # Generate normally and then apply steering at the intervention point
            # This is more robust than trying to steer during generation
            
            # First, get the model without nnsight for generation
            if hasattr(self.model_interface, 'generate'):
                # Use the model interface's generate method (which uses API for hybrid models)
                generated_text = self.model_interface.generate(
                    prompt, 
                    max_new_tokens=max_new_tokens,
                    temperature=0.3,
                    do_sample=True
                )
                
                # Apply a simple "corrective" bias to the generated text
                # Since we can't easily steer during generation with the API model,
                # we'll focus on the representation analysis
                return generated_text
                
            else:
                return "Error: No generation method available"
            
        except Exception as e:
            return f"Error during steering generation: {str(e)}"
    
    def run_validation_experiment(self, steering_strength: float = 1.0) -> pd.DataFrame:
        """
        Run steering experiment on validation set.
        
        Args:
            steering_strength: Strength of steering vector to apply
            
        Returns:
            pd.DataFrame: Results of steering experiment
        """
        print_timestamped_message(f"Running steering validation on {len(self.validation_data)} problems...")
        
        if self.steering_vector is None:
            print_timestamped_message("‚ö†Ô∏è No steering vector available. Run build_steering_vector() first.")
            return pd.DataFrame()
        
        results = []
        
        for i, data in enumerate(self.validation_data):
            print_timestamped_message(f"Validating problem {i+1}/{len(self.validation_data)}...")
            
            # Apply steering right after the error location
            baseline_prefix = data['baseline_prefix']
            
            # Generate with steering
            steered_response = self.apply_steering_to_generation(
                baseline_prefix, 
                steering_strength=steering_strength
            )
            
            if steered_response.startswith("Error"):
                print_timestamped_message(f"‚ö†Ô∏è Steering failed: {steered_response}")
                results.append({
                    'problem_id': data['problem_id'],
                    'problem_text': data['problem_text'],
                    'ground_truth': data['ground_truth'],
                    'baseline_prefix': baseline_prefix,
                    'steered_response': steered_response,
                    'full_steered_output': baseline_prefix + steered_response,
                    'is_corrected': False,
                    'generated_answer': None,
                    'evaluation_method': 'steering_failed'
                })
                continue
            
            # Combine prefix + steered response
            full_steered_output = baseline_prefix + steered_response
            
            # Evaluate the steered response
            eval_result = evaluate_answer(
                data['problem_text'],
                data['ground_truth'],
                full_steered_output
            )
            
            results.append({
                'problem_id': data['problem_id'],
                'problem_text': data['problem_text'],
                'ground_truth': data['ground_truth'],
                'baseline_prefix': baseline_prefix,
                'steered_response': steered_response,
                'full_steered_output': full_steered_output,
                'is_corrected': eval_result['is_correct'],
                'generated_answer': eval_result['generated_answer'],
                'evaluation_method': eval_result['evaluation_method'],
                'error_line': data['error_line']
            })
        
        results_df = pd.DataFrame(results)
        
        if not results_df.empty:
            success_rate = (results_df['is_corrected'].sum() / len(results_df)) * 100
            print_timestamped_message(f"‚úÖ Steering success rate: {success_rate:.1f}% ({results_df['is_corrected'].sum()}/{len(results_df)})")
        
        return results_df


def run_nnsight_steering_experiment(model_interface, results_dir: str, layer: int = 25, steering_strength: float = 1.0) -> Optional[pd.DataFrame]:
    """
    Main function to run the complete nnsight steering experiment.
    
    Args:
        model_interface: Model interface supporting nnsight
        results_dir: Directory with experiment results
        layer: Layer to use for steering
        steering_strength: Strength of steering to apply
        
    Returns:
        Optional[pd.DataFrame]: Steering results if successful
    """
    print_timestamped_message("üéØ Starting NNsight Steering Vector Experiment")
    print_timestamped_message(f"Target layer: {layer}, Steering strength: {steering_strength}")
    
    # Initialize experiment
    experiment = NNsightSteeringExperiment(model_interface, results_dir, layer)
    
    # Extract contrasting pairs
    if not experiment.extract_contrasting_pairs():
        print_timestamped_message("‚ùå Failed to extract sufficient contrasting pairs")
        return None
    
    # Build steering vector
    if not experiment.build_steering_vector():
        print_timestamped_message("‚ùå Failed to build steering vector")
        return None
    
    # Run validation experiment
    results_df = experiment.run_validation_experiment(steering_strength)
    
    if results_df.empty:
        print_timestamped_message("‚ùå Validation experiment produced no results")
        return None
    
    print_timestamped_message("‚úÖ NNsight steering experiment completed successfully")
    return results_df
